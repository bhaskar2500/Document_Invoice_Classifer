{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D ,Conv3D\n",
    "from keras.layers import Input, Dense, Activation ,Flatten\n",
    "from keras.layers import Reshape, Lambda \n",
    "from keras.layers.merge import add, concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model,Sequential\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image as ki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_input = Input(shape=(224, 224, 3))\n",
    "\n",
    "model = VGG16(input_tensor=image_input, include_top=True,weights='imagenet')\n",
    "\n",
    "last_layer = model.get_layer('block5_pool').output\n",
    "x= Flatten(name='flatten')(last_layer) \n",
    "x= Dense(128,activation='relu',name='fc1')(x)\n",
    "x= Dense(128,activation='relu',name='fc2')(x)\n",
    "out = Dense(2, activation='softmax', name='output')(x)\n",
    "custom_vgg_model = Model(image_input, out)\n",
    "\n",
    "\n",
    "for layer in custom_vgg_model.layers[:-3]:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "custom_vgg_model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_vgg_model.load_weights(\"InvoiceClassifier.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image as im\n",
    "import numpy as np\n",
    "size = (224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.772593e-22 1.000000e+00]]\n",
      "[[1. 0.]]\n",
      "[[1. 0.]]\n",
      "[[1. 0.]]\n",
      "[[1. 0.]]\n",
      "[[1. 0.]]\n",
      "[[1. 0.]]\n",
      "[[1. 0.]]\n",
      "[[1. 0.]]\n",
      "[[1. 0.]]\n",
      "[[1. 0.]]\n",
      "[[0. 1.]]\n",
      "[[0. 1.]]\n",
      "[[0. 1.]]\n",
      "[[0. 1.]]\n",
      "[[0. 1.]]\n",
      "[[0. 1.]]\n",
      "[[0. 1.]]\n",
      "[[0. 1.]]\n",
      "[[0. 1.]]\n",
      "[[0. 1.]]\n",
      "[[0. 1.]]\n",
      "[[0. 1.]]\n",
      "[[0. 1.]]\n",
      "[[0. 1.]]\n",
      "[[0. 1.]]\n",
      "[[0. 1.]]\n",
      "[[0. 1.]]\n",
      "[[0. 1.]]\n",
      "[[0. 1.]]\n",
      "[[0. 1.]]\n",
      "[[0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "prediction_dict = {}\n",
    "image_dir = \"mix\\\\\"\n",
    "import os\n",
    "for pic in os.listdir(image_dir):\n",
    "        img = im.open(image_dir+pic)\n",
    "        img =img.resize(size,) \n",
    "        img_array = ki.img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array,axis=0)\n",
    "        try:\n",
    "            prediction = custom_vgg_model.predict(img_array)\n",
    "            print(prediction)\n",
    "            prediction_dict[pic]= {\"type\":\"Invoice\",} if prediction.argmax() else  {\"type\":\"Flower\" }\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adhar.jpg': {'type': 'Invoice'},\n",
       " 'birth_month_flowers_primary_1920x1280px_pixabay.jpg': {'type': 'Flower'},\n",
       " 'butterfly_flower_01_hd_pictures_166973.jpg': {'type': 'Flower'},\n",
       " 'craig_burrows_uv_flowers_1.jpg': {'type': 'Flower'},\n",
       " 'Creative_Farmer_Red_Rose_Flower_SDL438762962_1_e60ab.jpeg': {'type': 'Flower'},\n",
       " 'dahlia_red_blossom_bloom_60597.jpeg': {'type': 'Flower'},\n",
       " 'daisy_pollen_flower_nature_87840.jpeg': {'type': 'Flower'},\n",
       " 'Dedicated_Love_large.jpg': {'type': 'Flower'},\n",
       " 'di9raM7nT.jpg': {'type': 'Flower'},\n",
       " 'dsc_0598_edit.jpg': {'type': 'Flower'},\n",
       " 'FCO_VAL_289.jpg': {'type': 'Flower'},\n",
       " 'pic_006.jpg': {'type': 'Invoice'},\n",
       " 'pic_007.jpg': {'type': 'Invoice'},\n",
       " 'pic_008.jpg': {'type': 'Invoice'},\n",
       " 'pic_010.jpg': {'type': 'Invoice'},\n",
       " 'pic_011.jpg': {'type': 'Invoice'},\n",
       " 'pic_012.jpg': {'type': 'Invoice'},\n",
       " 'pic_013.jpg': {'type': 'Invoice'},\n",
       " 'pic_017.jpg': {'type': 'Invoice'},\n",
       " 'pic_019.jpg': {'type': 'Invoice'},\n",
       " 'pic_022.jpg': {'type': 'Invoice'},\n",
       " 'pic_023.png': {'type': 'Invoice'},\n",
       " 'pic_026.jpg': {'type': 'Invoice'},\n",
       " 'pic_028.png': {'type': 'Invoice'},\n",
       " 'pic_029.jpg': {'type': 'Invoice'},\n",
       " 'pic_030.jpg': {'type': 'Invoice'},\n",
       " 'pic_033.jpg': {'type': 'Invoice'},\n",
       " 'pic_034.jpg': {'type': 'Invoice'},\n",
       " 'pic_035.jpg': {'type': 'Invoice'},\n",
       " 'pic_038.jpg': {'type': 'Invoice'},\n",
       " 'pic_040.jpg': {'type': 'Invoice'},\n",
       " 'pic_041.jpg': {'type': 'Invoice'}}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
